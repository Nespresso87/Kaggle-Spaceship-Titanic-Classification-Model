{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47a995-4ce4-4174-9a42-c907821503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "class SpaceshipPreprocessor:\n",
    "    \"\"\"Custom preprocessor for Spaceship Titanic data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.group_expenses = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    \n",
    "    def create_features(self, df):\n",
    "        \"\"\"Create advanced features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        df[self.group_expenses] = df[self.group_expenses].fillna(0)\n",
    "        for col in ['VIP', 'CryoSleep']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(False).astype(int)\n",
    "        \n",
    "        # Extract cabin information\n",
    "        if 'Cabin' in df.columns:\n",
    "            df[['Deck', 'Cabin_num', 'Side']] = df['Cabin'].str.split('/', expand=True)\n",
    "            df['Cabin_num'] = pd.to_numeric(df['Cabin_num'], errors='coerce')\n",
    "            df = df.drop('Cabin', axis=1)\n",
    "        \n",
    "        # Group features\n",
    "        df['TotalSpend'] = df[self.group_expenses].sum(axis=1)\n",
    "        df['HasSpent'] = (df['TotalSpend'] > 0).astype(int)\n",
    "        df['NumServicesUsed'] = (df[self.group_expenses] > 0).sum(axis=1)\n",
    "        df['AvgSpendPerService'] = df['TotalSpend'] / (df['NumServicesUsed'] + 1e-6)\n",
    "        \n",
    "        # Spending patterns\n",
    "        df['MaxSpend'] = df[self.group_expenses].max(axis=1)\n",
    "        df['SpendingVariety'] = df[self.group_expenses].apply(lambda x: (x > 0).sum(), axis=1)\n",
    "        \n",
    "        # Group size features\n",
    "        if 'Name' in df.columns:\n",
    "            df['LastName'] = df['Name'].fillna('').str.split().str[-1]\n",
    "            lastname_counts = df['LastName'].value_counts()\n",
    "            df['GroupSize'] = df['LastName'].map(lastname_counts)\n",
    "            df = df.drop(['Name', 'LastName'], axis=1)\n",
    "        \n",
    "        # Age-based features\n",
    "        if 'Age' in df.columns:\n",
    "            df['IsChild'] = (df['Age'] < 13).astype(int)\n",
    "            df['IsAdult'] = ((df['Age'] >= 13) & (df['Age'] < 65)).astype(int)\n",
    "            df['IsSenior'] = (df['Age'] >= 65).astype(int)\n",
    "            df['AgeBin'] = pd.qcut(df['Age'].fillna(df['Age'].median()), q=5, labels=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def create_model_pipeline(use_voting=True):\n",
    "    \"\"\"Create a pipeline with preprocessing and model\"\"\"\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = ['HomePlanet', 'Destination', 'Deck', 'Side']\n",
    "    numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
    "                     'Cabin_num', 'TotalSpend', 'NumServicesUsed', 'AvgSpendPerService',\n",
    "                     'MaxSpend', 'SpendingVariety', 'GroupSize']\n",
    "    binary_cols = ['VIP', 'CryoSleep', 'HasSpent', 'IsChild', 'IsAdult', 'IsSenior']\n",
    "    \n",
    "    # Create preprocessing pipelines\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_pipeline, numerical_cols),\n",
    "            ('cat', cat_pipeline, categorical_cols),\n",
    "            ('pass', 'passthrough', binary_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if use_voting:\n",
    "        # Create individual models\n",
    "        xgb = XGBClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            max_depth=7,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        lgbm = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=7,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Create voting classifier\n",
    "        model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb', xgb),\n",
    "                ('rf', rf),\n",
    "                ('lgbm', lgbm)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "    else:\n",
    "        model = XGBClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            max_depth=7,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "def optimize_hyperparameters(pipeline, X_train, y_train):\n",
    "    \"\"\"Perform hyperparameter optimization\"\"\"\n",
    "    \n",
    "    param_distributions = {\n",
    "        'classifier__xgb__learning_rate': uniform(0.01, 0.3),\n",
    "        'classifier__xgb__n_estimators': randint(50, 300),\n",
    "        'classifier__xgb__max_depth': randint(3, 10),\n",
    "        'classifier__xgb__subsample': uniform(0.6, 0.4),\n",
    "        'classifier__xgb__colsample_bytree': uniform(0.6, 0.4),\n",
    "        \n",
    "        'classifier__rf__n_estimators': randint(50, 300),\n",
    "        'classifier__rf__max_depth': randint(5, 20),\n",
    "        'classifier__rf__min_samples_split': randint(2, 10),\n",
    "        \n",
    "        'classifier__lgbm__learning_rate': uniform(0.01, 0.3),\n",
    "        'classifier__lgbm__n_estimators': randint(50, 300),\n",
    "        'classifier__lgbm__max_depth': randint(3, 10)\n",
    "    }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=50,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return random_search.best_estimator_\n",
    "\n",
    "def create_submission(train_df, test_df, use_optimization=True):\n",
    "    \"\"\"Create submission using the enhanced model pipeline\"\"\"\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = SpaceshipPreprocessor()\n",
    "    train_df = preprocessor.create_features(train_df)\n",
    "    test_df = preprocessor.create_features(test_df)\n",
    "    \n",
    "    # Save PassengerId for submission\n",
    "    test_passenger_ids = test_df['PassengerId']\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = train_df.drop(['PassengerId', 'Transported'], axis=1, errors='ignore')\n",
    "    y = train_df['Transported'].astype(int)\n",
    "    X_test = test_df.drop(['PassengerId'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipeline = create_model_pipeline(use_voting=True)\n",
    "    \n",
    "    if use_optimization:\n",
    "        # Split data for optimization\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Optimize hyperparameters\n",
    "        pipeline = optimize_hyperparameters(pipeline, X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "        print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Fit on full training data\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'PassengerId': test_passenger_ids,\n",
    "        'Transported': test_predictions.astype(bool)\n",
    "    })\n",
    "    \n",
    "    return submission_df, val_accuracy if use_optimization else None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    \n",
    "    print(\"\\nCreating submission with optimized model...\")\n",
    "    submission_df, accuracy = create_submission(train_df, test_df, use_optimization=True)\n",
    "    \n",
    "    print(\"\\nSaving submission file...\")\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission file info:\")\n",
    "    print(f\"Total predictions: {len(submission_df)}\")\n",
    "    print(f\"Percentage True: {(submission_df['Transported'] == True).mean():.2%}\")\n",
    "    print(f\"Percentage False: {(submission_df['Transported'] == False).mean():.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
